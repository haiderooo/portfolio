---
title: "How to Check OCR Output Quality"
excerpt_separator: "<!--more-->"
categories:
  - project1
tags:
  - experiments
  - tests
image: images/Eval_OCR_banner.jpg
---

Here's how you tell if an OCR model is working well enough on your text.
<!--more-->

Hello digihuman! So you familiarized yourself with training models with segmentation and transcription masala.
But did you taste the dish before serving? Now you need to know if your model is good.
It is not enough to look at the result visually. You need to correctly evaluate an OCR Output for quality check.


The key metrics to keep in mind here are: Character Error Rate (CER) and Word Error Rate (WER). 
Stretching the cooking analogy unnecessarily, you have to check for both the taste of curry and the meat (or the veggies if you refuse to eat flesh). 
Okay apologies, let’s dive in.


But first we need to know how errors are measured. This is accounted for through Error Rate (ER) types and the Levenshtein Distance. 


You could go for a 1 for accurate and a 0 for non-accurate, making it boolean and sexy. 
But that is not enough data to make decisions on. That’s why we go for ERs instead.


There are three ER types that tell different ways an OCR transcribed text is off from the ground truth text. 
First is substitution error (S) referring to misspelling, then comes deletion error (D) which is lost/missing chars/words, and 
finally inclusion error (I) which means words/chars included incorrectly. 


![a random image]({{site.baseurl}}/images/Eval_OCR_ER types.webp)


Then we use Levenshtein distance which simply is the minimum number of single character edits needed to correct a word or a sentence.


**CER**


Based on the types of errors (S, D and I) and the levenshtein distance you can calculate CER through the following equation: 


CER=(S+D+I)/N where N represents the number of characters in the text. 


CER would then be the percentage of errors in a text with a 0 meaning no character errors found.


For example, for a ground truth text “809475127”, if OCR transcribed text is 80g475Z7:


S = 2
D =  1
I = 0


And CER would be: 2+1+0/9 = 0.333 which means that 33% of the text is mistranscribed.


Too many inclusion errors can cause the CER value to go beyond 100% which then has to be normalized through the following equation: 


CERnormalized = (S+D+I)/(S+D+I+C) where C is the number of correct characters. 


There is no umbrella benchmark but generally a good CER level is 1-2%, average level 2-10% and poor beyond 10%.


WER


An WER check makes sense for paragraphs and multiple paragraphs. 
Otherwise CER is sufficient for a couple of sentences worth of characters or less. 
The equation for WER would is:


WER = (Sw + Dw + Iw)/N. 


Generally WER and CER are correlated in a text whereas WER is expected to be greater than CER. 


In terms of practicalities, WER and CER can be calculated in python through a package such as fastwer and manually labelled ground truth text. 
But more on that later. 


I am sure it wasn’t too tough to understand this but 
things would make a 100 percent sense (or with 0% error rate) after some practice with the equations and identifying errors. 
Best of luck! See you soon!


----------------------------------------------------------

**References:**


Leung, K. (2021, June 24). Evaluate OCR Output Quality with Character Error Rate (CER) and Word Error Rate (WER). Towards Data Science. 
https://towardsdatascience.com/evaluating-ocr-output-quality-with-character-error-rate-cer-and-word-error-rate-wer-853175297510/

